{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ### Note on Labs and Assignments:\n",
        ">\n",
        "> üîß Look for the **wrench emoji** üîß ‚Äî it highlights where you're expected to take action!\n",
        ">\n",
        "> These sections are graded and are not optional.\n",
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IS 4487 Lab 13: Sentiment Analysis\n",
        "\n",
        "## Outline\n",
        "\n",
        "- Analyze a dataset utilizing sentiment\n",
        "- Compare the VADER and TextBlob models\n",
        "- Learn the basics of sentiment analysis\n",
        "\n",
        "In this lab, you will explore **sentiment analysis** techniques to determine the positivity/negativity of certain sentences. \n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/Stan-Pugsley/is_4487_base/blob/main/Labs/lab_13_text_analytics.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Description\n",
        "\n",
        "We will use a dataset containing movie reviews.  Each review is contained in the \"Phrase\" variable.  This dataset is pre-labeled with the Sentiment, but we will use tools to calculate our own Sentiment, which could then be compared to the pre-labeled values.  \n",
        "\n",
        "The dataset does not specify the movie that it is reviewing.  We wouldn't be able to use this to calculate a movie score, like Rotten Tomatoes.  But we can observe the overall sentiment of reviewers and practice using the text analytics tools available in Python.\n",
        "\n",
        "| Column                        | Data Type       | Description                                                  |\n",
        "|------------------------------|------------------|--------------------------------------------------------------|\n",
        "| `PhraseID`                   | Integer           | ID of an entry                                               |\n",
        "| `SentenceID`                 | Integer           | Shows which phrases belong to which sentence                                      |\n",
        "| `Phrase`             | String       | A sentence/phrase                       |\n",
        "| `Sentiment`                 | Categorical       | 0 = Very Negative, 1 = Negative, 2 = Neutral, 3 = Positive, 4= Highly Positive        |\n",
        "\n",
        "Source: https://www.kaggle.com/datasets/satwikdondapati/moviereviewsentimentalanalysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Load and Prepare the Data\n",
        "\n",
        "### What you are going to do:\n",
        "- Load the dataset\n",
        "- Preview the data \n",
        "\n",
        "### Why this matters:\n",
        "All throughout the semester you've mainly dealt with data that had a wide variety of types. But what if we only have a few variables and one of them has tons of data?\n",
        "\n",
        "**Things to notice:**\n",
        "- Which variables are actually important? \n",
        "- Why are there so few variables? Which variable(s) has the most data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß Try It Yourself\n",
        "Import the libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# Load the reviews\n",
        "url = \"https://raw.githubusercontent.com/Stan-Pugsley/is_4487_base/refs/heads/main/DataSets/move_reviews.tsv\"\n",
        "df = pd.read_csv(url, sep='\\t', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2 : See the Scores\n",
        "### What you are going to do:\n",
        "- count how many entries belong to each Sentiment score\n",
        "- create a visualization of the value counts \n",
        "\n",
        "### Why this matters:\n",
        "In the dataset you'll notice that each phrase has a sentiment score. These scores tell us whether a phrase is positive or negative or neutral in tone. In later steps we will be predicting scores, so it is important to know if there is any skew of sentiment within the data.  \n",
        "\n",
        "**Things to notice:**\n",
        "- Why are there 5 total Sentiment scores?\n",
        "- What does each score mean?\n",
        "- What is the count of each score? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß Try It Yourself ‚Äî Part 2\n",
        "\n",
        "1. create a variable that stores the ```value_counts``` of the Sentiment column\n",
        "2. create a bar chart using the ```value_counts``` variable you just created\n",
        "3. make a comment about the chart. Is there any skew within the data? Is it more positive, negative, or neutral? (Hint: refer to the data dictionary at the very top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Enter your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: VADER\n",
        "### What you are going to do:\n",
        "- run and train a VADER model\n",
        "- evaluate its performance \n",
        "\n",
        "### Why this matters:\n",
        "VADER is a rule based model great for handling short sentences and phrases. It works by looking at each word individually and assigning an individual score to it. These scores then get compounded at the end of its calculation and it generates a sentiment score. Since it looks at one word at a time it may struggle with longer sentences. So VADER is best suited for analyzing social media and reviews. \n",
        "\n",
        "**Things to notice:**\n",
        "- what does the score_to_label function do? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set up and run the VADER model \n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "df[\"vader_score\"] = [sia.polarity_scores(text)[\"compound\"] for text in df[\"Phrase\"]]\n",
        "\n",
        "# there are a total of 5 sentiment scores but polarity only gives us scores between -1 to 1\n",
        "# need to map the scores from the dataset so that polarity can use it \n",
        "def score_to_label(score):\n",
        "    if score <= -0.6:\n",
        "        return 0  # very negative\n",
        "    elif score <= -0.2:\n",
        "        return 1  # somewhat negative\n",
        "    elif score < 0.2:\n",
        "        return 2  # neutral\n",
        "    elif score < 0.6:\n",
        "        return 3  # somewhat positive\n",
        "    else:\n",
        "        return 4  # very positive\n",
        "\n",
        "df[\"vader_pred\"] = df[\"vader_score\"].apply(score_to_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß Try It Yourself ‚Äî Part 3\n",
        "Now we know if the data is positively, negatively, or neutrally skewed. For the next two steps we are going to be testing out 2 new models and comparing their results. \n",
        "1. using ```vader_pred``` and ```Sentiment``` generate a classification report \n",
        "2. using ```vader_pred``` and ```Sentiment``` generate a confusion matrix\n",
        "3. make a comment. Why are the values for a sentiment score of 2 much higher than all the others? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Add code here "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: TextBlob\n",
        "### What you are going to do:\n",
        "- evaluate the performance of a TextBlob model \n",
        "\n",
        "## Why this matters:\n",
        "Unlike VADER, TextBlod utilizes tokenization to determine the sentiment of a phrase. Tokenization is great for breaking up large sentences and paragraphs. Because of this, TextBlob works better on longer sentences and is best suited for analyzing longer text documents such as articles or blogs.\n",
        "\n",
        "**Things to notice:**\n",
        "- how does accuracy compare to the previous model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# set up and run the textblob model \n",
        "df[\"textblob_score\"] = [TextBlob(text).sentiment.polarity for text in df[\"Phrase\"]]\n",
        "df[\"textblob_pred\"] = df[\"textblob_score\"].apply(score_to_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîß Try It Yourself ‚Äî Part 4\n",
        "Now let's evaluate the TextBlob model.\n",
        "1. using ```textblob_pred``` and ```Sentiment``` generate a classification report \n",
        "2. using ```textblob_pred``` and ```Sentiment``` generate a confusion matrix\n",
        "3. make a comment. Why are the values for a sentiment score of 2 much higher than all the others? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Enter your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Part 5: Reflection (100 words or less)\n",
        "\n",
        "In this lab you built a VADER and TextBlob model and evaluated their results. You also learned about some of the pros and cons of each model and in which situations they would be used. \n",
        "\n",
        "Use the cell below to answer the following questions:\n",
        "\n",
        "1. In the dataset there was a certain sentiment score that had an extremely high frequency. How did the high number of frequency for this score affect its associated metrics? \n",
        "1. Which model had better accuracy in this lab? Why is that the case? (Hint: look at the results of head() at the very top of this lab. How long are the phrases in this dataset?) \n",
        "2. Why is it important for a business to determine sentiment? How could a business use sentiment analysis and customer reviews to improve customer service? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úçÔ∏è Your Response: üîß\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export Your Notebook to Submit in Canvas\n",
        "Use the instructions from Lab 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!jupyter nbconvert --to html \"lab_13_LastnameFirstname.ipynb\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
