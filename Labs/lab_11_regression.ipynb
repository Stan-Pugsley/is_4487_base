{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "> # ‚ö†Ô∏è **IMPORTANT: READ BEFORE STARTING THIS LAB**\n",
        ">\n",
        "> ### Throughout this lab, you will see **üîß Try It Yourself** sections and a final üîß **Reflection** section\n",
        ">\n",
        "> ‚úÖ You are expected to:\n",
        "> - Complete each **\"üîß Try It Yourself‚Äù** section by writing and running your own code or answering the prompted questions in a markdown or python cell below the section.\n",
        "> - Answer the **Reflection** section at the end of the lab in your own words. This is your opportunity to summarize what you learned and connect the concepts.\n",
        "\n",
        ">\n",
        ">\n",
        "> üîß Look for the **wrench emoji** üîß ‚Äî it highlights where you're expected to take action!\n",
        ">\n",
        "> ### These sections are **graded** and are **not optional**. Skipping them will impact your lab score.\n",
        ">\n",
        "> ---"
      ],
      "metadata": {
        "id": "MGF9KN3311v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 11: Regression\n",
        "\n",
        "In this lab, we‚Äôll use the merged HR dataset to build a **logistic regression model** to predict whether an employee is likely to leave the company (i.e., attrition).\n",
        "\n",
        "### Objectives:\n",
        "1. Load and explore the dataset\n",
        "2. Clean and prepare features\n",
        "3. Encode categorical variables\n",
        "4. Split the data into training and test sets\n",
        "5. Train and evaluate a logistic regression model\n",
        "6. Reflect on variable importance and model fit\n",
        "\n",
        "**Target Variable:** `Attrition` (Yes/No)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/Stan-Pugsley/is_4487_base/blob/main/Labs/lab11_regression_v2.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ],
      "metadata": {
        "id": "mjw0KP6b2M9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Overview\n",
        "\n",
        "**Dataset:** `merged_hr_data.csv`  \n",
        "Source: [Kaggle HR Analytics Case Study](https://www.kaggle.com/datasets/vjchoudhary7/hr-analytics-case-study)\n",
        "\n",
        "| Variable                      | Type        | Description |\n",
        "|-------------------------------|-------------|-------------|\n",
        "| `Age`                         | Numeric     | Age of the employee |\n",
        "| `Attrition`                   | Categorical | Whether the employee has left the company (Yes/No) |\n",
        "| `BusinessTravel`              | Categorical | Frequency of business travel |\n",
        "| `Department`                  | Categorical | Department name |\n",
        "| `DistanceFromHome`           | Numeric     | Distance from home to work (in km) |\n",
        "| `Education`                  | Ordinal     | Employee education level (1‚Äì5) |\n",
        "| `EducationField`             | Categorical | Field of education |\n",
        "| `EmployeeID`                 | Identifier  | Unique identifier for employee |\n",
        "| `EmployeeCount`              | Constant    | Always 1 (not useful for modeling) |\n",
        "| `EnvironmentSatisfaction`    | Ordinal     | Satisfaction with the environment (1‚Äì4) |\n",
        "| `Gender`                     | Categorical | Gender of the employee |\n",
        "| `JobInvolvement`             | Ordinal     | Level of involvement with job (1‚Äì4) |\n",
        "| `JobLevel`                   | Ordinal     | Employee level (1‚Äì5) |\n",
        "| `JobRole`                    | Categorical | Job title |\n",
        "| `JobSatisfaction`            | Ordinal     | Satisfaction with the job (1‚Äì4) |\n",
        "| `MaritalStatus`              | Categorical | Marital status |\n",
        "| `MonthlyIncome`              | Numeric     | Monthly salary in USD |\n",
        "| `NumCompaniesWorked`         | Numeric     | Number of companies previously worked for |\n",
        "| `Over18`                     | Constant    | Always \"Y\" (not useful) |\n",
        "| `PercentSalaryHike`          | Numeric     | Percentage salary increase |\n",
        "| `PerformanceRating`          | Ordinal     | Performance rating (1‚Äì4) |\n",
        "| `StandardHours`              | Constant    | Always 80 (not useful) |\n",
        "| `StockOptionLevel`           | Ordinal     | Stock options level (0‚Äì3) |\n",
        "| `TotalWorkingYears`          | Numeric     | Total years of professional experience |\n",
        "| `TrainingTimesLastYear`      | Numeric     | Number of training sessions attended last year |\n",
        "| `WorkLifeBalance`            | Ordinal     | Work-life balance rating (1‚Äì4) |\n",
        "| `YearsAtCompany`             | Numeric     | Years spent at the current company |\n",
        "| `YearsInCurrentRole`         | Numeric     | Years spent in current role |\n",
        "| `YearsSinceLastPromotion`    | Numeric     | Years since last promotion |\n",
        "| `YearsWithCurrManager`       | Numeric     | Years with current manager |\n",
        "| `JobSatisfaction`            | Ordinal     | Self-reported job satisfaction (1‚Äì4) |\n",
        "| `EnvironmentSatisfaction`    | Ordinal     | Satisfaction with the work environment (1‚Äì4) |\n",
        "| `WorkLifeBalance`            | Ordinal     | Work-life balance rating (1‚Äì4) |\n",
        "| `JobInvolvement`             | Ordinal     | Employee‚Äôs job involvement level (1‚Äì4) |\n",
        "| `PerformanceRating`          | Ordinal     | Most recent performance rating |\n",
        "\n"
      ],
      "metadata": {
        "id": "7EenZ8qDCZnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Load data and packages\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "63A8bxtoDHZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the merged HR dataset\n",
        "url = \"https://raw.githubusercontent.com/Stan-Pugsley/is_4487_base/fb26c9731228bfbdefe94143ac53ef5500a199e8/DataSets/merged_hr_data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Preview structure\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "LBi6IsGR2MZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Cleaning\n",
        "\n",
        "Real-world HR data often contains administrative fields (e.g., ID numbers), constants (same value for all rows), or missing values.\n",
        "\n",
        "### What We‚Äôre Doing:\n",
        "- Remove irrelevant or constant columns: `EmployeeCount`, `Over18`, `StandardHours`, `EmployeeID`\n",
        "- Drop rows with missing data\n",
        "\n",
        "### Why It Matters:\n",
        "- Non-informative or redundant features can reduce model accuracy and interpretability.\n",
        "- Logistic regression does not handle missing values natively, so we need a clean dataset.\n",
        "- Dropping some rows is reasonable here due to the relatively small number of nulls.\n",
        "\n",
        "> Ethical Note: In practice, dropping rows may disproportionately exclude certain groups‚Äîso this step should be handled with caution.\n"
      ],
      "metadata": {
        "id": "pCXh3rZEDDqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKbOk3wnyI6x"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns\n",
        "drop_cols = ['EmployeeCount', 'Over18', 'StandardHours', 'EmployeeID']\n",
        "df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "# Drop rows with any missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Check result\n",
        "print(\"After cleaning:\", df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Encode Categorical Variables\n",
        "\n",
        "Machine learning algorithms like logistic regression require **numeric inputs**. To use categorical data like `Gender` or `JobRole`, we convert them into **dummy variables** using one-hot encoding.\n",
        "\n",
        "### Key Steps:\n",
        "- Convert target `Attrition` to 1 (Yes) and 0 (No)\n",
        "- Use `pd.get_dummies()` with `drop_first=True` to avoid multicollinearity\n",
        "\n",
        "### Why It Matters:\n",
        "- Ensures model can interpret categorical inputs numerically\n",
        "- Dropping the first dummy prevents the \"dummy variable trap\" where one variable is a linear combination of others\n",
        "- Accurate encoding helps ensure model fairness and interpretability\n",
        "\n",
        "> Reminder: Avoid encoding identifiers or columns with too many unique levels without reduction.\n",
        "\n"
      ],
      "metadata": {
        "id": "ELOsb3sJDieS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary encode target\n",
        "df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# One-hot encode categorical features\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Preview encoded columns\n",
        "df_encoded.columns"
      ],
      "metadata": {
        "id": "xi1kZ7ewDhfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Try It Yourself - Part 3\n",
        "\n",
        "1. How many new columns were created during one-hot encoding?  \n",
        "2. Why is it important to avoid including columns like `EmployeeID` in modeling?\n"
      ],
      "metadata": {
        "id": "Kh7PpONDDrVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Add comment here:"
      ],
      "metadata": {
        "id": "5tRDHPJbD8yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Standardizing Features for Logistic Regression\n",
        "\n",
        "When using models like **logistic regression**, it's important to ensure all numeric features are on a similar scale. This helps the model converge more reliably and prevents features with larger magnitudes from dominating the learning process.\n",
        "\n",
        "In this step, we'll use `StandardScaler` from `sklearn` to scale all feature columns to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "This is especially important when:\n",
        "- You're using gradient-based algorithms (like logistic regression, SVM, or neural networks)\n",
        "- Your dataset includes variables with vastly different units or scales (e.g., \"Age\" vs. \"MonthlyIncome\")\n",
        "\n",
        "> **Note:** The target variable (`Attrition`) should **not** be scaled ‚Äî only the input features.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "baWjA4QAcq3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Separate the features and the target\n",
        "X = df_encoded.drop(columns=['Attrition'])\n",
        "y = df_encoded['Attrition']\n",
        "\n",
        "# Apply standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reconstruct scaled DataFrame\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns)\n"
      ],
      "metadata": {
        "id": "GqJpzo0TcqQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Try It Yourself - Part 4\n",
        "\n",
        "You've now scaled your features using `StandardScaler`, which makes each feature have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "**Think about this:**\n",
        "Suppose we didn't standardize the features and trained a logistic regression model using raw input data instead. What might happen to:\n",
        "- The convergence of the model (would it complete successfully?)\n",
        "- The interpretation or relative importance of the coefficients?\n",
        "\n",
        "**Write one or two sentences** explaining how not standardizing the data could affect the model's performance or interpretability.\n"
      ],
      "metadata": {
        "id": "FEsXIv2ddXgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Add comment here:"
      ],
      "metadata": {
        "id": "ScvQyZZsdYiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Train-Test Split\n",
        "\n",
        "We'll split the dataset into:\n",
        "- 80% for training\n",
        "- 20% for testing\n",
        "\n",
        "To preserve class proportions, we **stratify on `Attrition`**. This ensures fair evaluation.\n",
        "\n",
        "> This step helps avoid training/test imbalance especially in classification tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "WijlP1IqDvig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use already standardized features in X, and original target y\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Check the shapes of the splits\n",
        "X_train.shape, X_test.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "HyxzBtcWD3So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Try It Yourself - Part 5\n",
        "\n",
        "1. What percentage of each class (Yes/No) is in your training set?\n",
        "2. Why is stratified sampling especially important for classification?\n"
      ],
      "metadata": {
        "id": "DBVnTW3SD7YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Add code here"
      ],
      "metadata": {
        "id": "BP5ri11tD6pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Add comment here:"
      ],
      "metadata": {
        "id": "5pE6rsE5EEJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Train Logistic Regression\n",
        "\n",
        "Now we fit a logistic regression model using the training data. This model estimates the **probability** of attrition given employee characteristics.\n",
        "\n",
        "> Logistic regression is simple, interpretable, and a great starting point.\n"
      ],
      "metadata": {
        "id": "4gIDp7J0EDly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and fit logistic model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Display top predictors\n",
        "import numpy as np\n",
        "coefficients = pd.Series(model.coef_[0], index=X_train.columns)\n",
        "coefficients.sort_values(ascending=False).head(10)\n"
      ],
      "metadata": {
        "id": "K63xKIS5H2kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Try It Yourself - Part 6\n",
        "\n",
        "1. Which features are most positively associated with attrition?\n",
        "2. Which features are most negatively associated with staying?"
      ],
      "metadata": {
        "id": "kWqpju5JH9QG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Add comment here:"
      ],
      "metadata": {
        "id": "O8qEItqL2ViE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Evaluate Model Performance\n",
        "\n",
        "Let‚Äôs test how well our model generalizes to unseen data. We'll compute:\n",
        "- Accuracy\n",
        "- Confusion matrix\n",
        "- Precision, recall, F1-score\n",
        "\n",
        "> These metrics show how well we balance false positives vs. false negatives.\n"
      ],
      "metadata": {
        "id": "g7aMoWFTIELd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "wIwpnLp1H6US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Try It Yourself - Part 7\n",
        "\n",
        "1. Is the model better at predicting ‚ÄúYes‚Äù (leavers) or ‚ÄúNo‚Äù (stayers)?\n",
        "2. Do you think accuracy is the best metric here?"
      ],
      "metadata": {
        "id": "2J1SQjG9IYug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Add comment here:"
      ],
      "metadata": {
        "id": "efDTagytJGXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Feature Selection for Accuracy Improvement\n",
        "\n",
        "Not all features equally influence attrition. By identifying and using only the most important predictors, we can:\n",
        "- Simplify the model\n",
        "- Potentially improve performance or interpretability\n",
        "- Reduce overfitting\n",
        "\n",
        "We‚Äôll use the logistic regression model‚Äôs coefficients to rank feature importance.\n"
      ],
      "metadata": {
        "id": "olFZzAwzJKUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top 10 features based on absolute coefficient magnitude\n",
        "top_features = coefficients.abs().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Print the top features and their weights\n",
        "top_features"
      ],
      "metadata": {
        "id": "MGlsrGrHIKEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Try It Yourself ‚Äì Part 8\n",
        "\n",
        "1. Create a new training and test set using only the 10 most important features.\n",
        "2. Retrain the logistic regression model on this reduced dataset.\n",
        "3. Evaluate performance (accuracy, confusion matrix, classification report).\n"
      ],
      "metadata": {
        "id": "aQf0yeujJYdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Step 1: Identify the top N most important features using absolute value of logistic regression coefficients\n",
        "\n",
        "# üîß Step 2: Create new versions of X_train and X_test with only those top features\n",
        "\n",
        "# üîß Step 3: Initialize and fit a new LogisticRegression model on the reduced feature set\n",
        "\n",
        "# üîß Step 4: Use the new model to predict on the test set\n",
        "\n",
        "# üîß Step 5: Evaluate the reduced model using accuracy, confusion matrix, and classification report"
      ],
      "metadata": {
        "id": "yJKGxi_vJZ3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Part 9: Reflection\n",
        "\n",
        "1. How did the reduced-feature model compare to the full model?\n",
        "2. Would this version be easier to explain or use in an HR meeting?\n"
      ],
      "metadata": {
        "id": "4WTUyNSZJqOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Add comment here:"
      ],
      "metadata": {
        "id": "0vWE_lz0Jx5j"
      }
    }
  ]
}
