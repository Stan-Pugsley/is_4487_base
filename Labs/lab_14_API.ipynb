{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNv6tSwcyFfM"
      },
      "source": [
        "> ### Note on Labs and Assigments:\n",
        ">\n",
        "> Throughout this lab, you will see **🔧 Try It Yourself** sections and a final 🔧 **Reflection** section\n",
        ">\n",
        "> ✅ You are expected to:\n",
        "> - Complete each **\"🔧 Try It Yourself”** section by writing and running your own code or answering the prompted questions in a markdown or python cell below the section.\n",
        "> - Answer the **Reflection** section at the end of the lab in your own words. This is your opportunity to summarize what you learned and connect the concepts.\n",
        ">\n",
        "> 🔧 Look for the **wrench emoji** 🔧 — it highlights where you're expected to take action!\n",
        ">\n",
        "> These sections are **graded** and are **not optional**. \n",
        ">\n",
        "> ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldFBjQhfy6RN"
      },
      "source": [
        "# IS 4487 Lab 14: Reddit API\n",
        "\n",
        "## Outline\n",
        "\n",
        "- Retrieve Reddit post titles using the Pushshift API  \n",
        "- Clean and prepare text data for analysis  \n",
        "- Apply TF-IDF and K-Means clustering to discover major discussion topics  \n",
        "- Visualize topic clusters using word clouds  \n",
        "- Reflect on business use cases for topic modeling  \n",
        "\n",
        "In this lab, you will collect Reddit data from a business-focused subreddit and apply **unsupervised learning** to identify emerging themes and public sentiment. This type of analysis is valuable in marketing, finance, and product strategy for uncovering real-time customer interests and concerns.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/Stan-Pugsley/is_4487_base/blob/main/Labs/lab_14_API.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kiDKiXdy6JG"
      },
      "source": [
        "## Part 1: Reddit API Setup (via PRAW)\n",
        "\n",
        "Reddit's official API requires authentication. Follow these steps:\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 Get Your API Credentials\n",
        "\n",
        "1. Go to: https://www.reddit.com/prefs/apps\n",
        "2. Login or create an account\n",
        "3. Click the \"are you a developer? create an app\" button on the page\n",
        "4. Fill out:\n",
        "   - Name: `lab14`\n",
        "   - Type: **script**\n",
        "   - Redirect URI: `http://localhost`\n",
        "   - Leave the other fields blank\n",
        "5. After submitting:\n",
        "   - **client_id**: the 14-character string below your app name and \"personal use script\"\n",
        "   - **client_secret**: the chain of numbers next to secret\n",
        "\n",
        "---\n",
        "\n",
        "Paste your credentials in the Python cell below to connect to Reddit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACoowIO7y3Qy"
      },
      "outputs": [],
      "source": [
        "!pip install praw\n",
        "import praw\n",
        "\n",
        "# Replace these with your own credentials\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"ENTER CLIENT ID\",\n",
        "    client_secret=\"ENTER CLIENT SECRET\",\n",
        "    user_agent=\"lab14-reddit-topic-model\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ui8b34Bimf0"
      },
      "source": [
        "## Part 2: Collect Reddit Post Titles\n",
        "\n",
        "Let’s pull the latest 100 posts from the `r/stocks` subreddit.\n",
        "\n",
        "We'll store the `title` and `created_utc` fields in a DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohoWCcByfucw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Choose a subreddit and fetch posts\n",
        "subreddit_name = \"stocks\"\n",
        "posts = []\n",
        "\n",
        "for submission in reddit.subreddit(subreddit_name).hot(limit=100):\n",
        "    posts.append([submission.title, submission.created_utc])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(posts, columns=[\"title\", \"created_utc\"])\n",
        "df.head(20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9XOipMIitY3"
      },
      "source": [
        "### 🔧 Try It Yourself - Part 2\n",
        "\n",
        "1. Copy and paste the code above and **change the subreddit name** to one that matches your **future career goal** or your **current major/minor**. Here are some popular business-related subreddits you can choose from:\n",
        "\n",
        "   - `r/Entrepreneur`\n",
        "   - `r/business`\n",
        "   - `r/startups`\n",
        "   - `r/Finance`\n",
        "   - `r/smallbusiness`\n",
        "   - `r/marketing`\n",
        "   - `r/investing`\n",
        "   - `r/consulting`\n",
        "\n",
        "   👉 [Explore more subreddits here](https://www.reddit.com/subreddits/)\n",
        "\n",
        "2. **Update the code to increase the number of posts pulled by changing the `limit` parameter to `200`.**  \n",
        "\n",
        "   Then, run the following lines of code to see how many total posts you collected and how many duplicate titles there are:\n",
        "\n",
        "```\n",
        "   print(f\"Total posts: {len(df)}\")\n",
        "   print(df['title'].duplicated().sum(), \"duplicates\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq-eFfC_ykqa"
      },
      "outputs": [],
      "source": [
        "# 🔧 Add code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmp4ik6i17U"
      },
      "source": [
        "## Part 3: Clean and Prepare the Text\n",
        "\n",
        "Next, we’ll:\n",
        "- Lowercase the text\n",
        "- Remove links, punctuation, and extra spaces\n",
        "\n",
        "Cleaned text will be passed to the vectorizer in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzgISqenjHLX"
      },
      "outputs": [],
      "source": [
        "import re  # Import the regular expressions module for text pattern matching\n",
        "\n",
        "# Define a function to clean text data\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert all text to lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove punctuation, numbers, and special characters (keep only letters and spaces)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Replace multiple spaces with a single space and strip leading/trailing spaces\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to each post title in the DataFrame\n",
        "df['clean_title'] = df['title'].apply(clean_text)\n",
        "\n",
        "# Display the first few rows of the updated DataFrame\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h02NOp6AjKH8"
      },
      "source": [
        "### 🔧 Try It Yourself - Part 3\n",
        "\n",
        "1. **Create a new column to count the number of words in each post title.**  \n",
        "   This uses the cleaned title column (`clean_title`) to calculate word count:\n",
        "\n",
        "```\n",
        "   df['word_count'] = df['clean_title'].apply(lambda x: len(x.split()))\n",
        "```\n",
        "2. **Filter the DataFrame** to remove any rows where the post title has fewer than 3 words.\n",
        "\n",
        "3. **Sort the DataFrame** by the `word_count` column in descending order and display the top 5 longest post titles.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftrnTYZEjbv8"
      },
      "outputs": [],
      "source": [
        "# 🔧 Add code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfZcULXujdRb"
      },
      "source": [
        "## Part 4: TF-IDF and KMeans Clustering\n",
        "\n",
        "Now, we’ll:\n",
        "- Vectorize the cleaned text using TF-IDF\n",
        "- Apply KMeans to group posts by topic\n",
        "\n",
        "Clustering lets us see which themes dominate Reddit discussions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAeg7ntFjf-R"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text data into numerical features\n",
        "from sklearn.cluster import KMeans  # For performing K-Means clustering\n",
        "\n",
        "# Create a TF-IDF vectorizer that removes English stop words and limits features to the top 1,000 terms\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "\n",
        "# Transform the cleaned titles into a TF-IDF matrix (rows = posts, columns = terms)\n",
        "X = vectorizer.fit_transform(df['clean_title'])\n",
        "\n",
        "# Initialize and fit a K-Means model with 4 clusters\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "\n",
        "# Predict cluster assignments and add them as a new column to the DataFrame\n",
        "df['cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "# Display the first few cleaned titles along with their assigned cluster\n",
        "df[['clean_title', 'cluster']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg7CjLC6jjs2"
      },
      "source": [
        "### 🔧 Try It Yourself - Part 4\n",
        "\n",
        "1. **Change the number of clusters from 4 to 5** in your clustering code (e.g., if you're using KMeans).\n",
        "\n",
        "2. **Check how many posts were assigned to each cluster** by counting the values in the `cluster` column:\n",
        "\n",
        "\n",
        "3. **Observe and describe one change** that occurred when you increased the number of clusters from 4 to 5.  \n",
        "   This could include shifts in cluster size, new topic groupings, or changes in how the posts are categorized.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irIN1q-5y7ID"
      },
      "outputs": [],
      "source": [
        "# 🔧 Add code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icCNxs2by8X8"
      },
      "source": [
        "🔧 Add comment here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqB-_NJ6jpLJ"
      },
      "source": [
        "## Part 5: Top Words per Cluster\n",
        "\n",
        "This step shows the 10 most important words in each cluster using TF-IDF scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM-uJQDfjsje"
      },
      "outputs": [],
      "source": [
        "# Get the list of feature names (i.e., the terms used in the TF-IDF matrix)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Get the indices of terms with the highest weights in each cluster center\n",
        "# argsort() sorts each cluster center's weights in ascending order; [::-1] reverses to descending\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "# Loop through each cluster to print the top 10 keywords\n",
        "for i in range(kmeans.n_clusters):\n",
        "    print(f\"\\nCluster {i} keywords:\")  # Label for the current cluster\n",
        "    # Print the top 10 terms with the highest TF-IDF scores for this cluster\n",
        "    print(\", \".join([terms[ind] for ind in order_centroids[i, :10]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwIvqARwjv8R"
      },
      "source": [
        "### 🔧 Try It On Your Own – Part 6Word Clouds by Cluster\n",
        "\n",
        "Let’s visualize the dominant words in each cluster using **word clouds**. This will help you interpret what each cluster represents based on the most frequent terms.\n",
        "\n",
        "### Instructions:\n",
        "1. Copy and paste the code below in a python cell. Fill in the blanks to generate a word cloud for each cluster of Reddit post titles.\n",
        "\n",
        "\n",
        "```\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(____):  # Loop over each cluster number\n",
        "    texts = df[df['cluster'] == ____]['clean_title']  # Filter titles for the current cluster\n",
        "    combined = \" \".join(____)  # Combine all titles into one string\n",
        "    \n",
        "    wc = WordCloud(background_color=\"white\", max_words=100).generate(____)\n",
        "    \n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.title(f\"Cluster {i}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "```\n",
        "\n",
        "2. In 1–2 sentences, write an observation for 2 wordclouds.\n",
        "\n",
        "> What common theme or topic does this cluster seem to represent?\n",
        "\n",
        "> Is there anything surprising or unclear about the cluster?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyAO3JGMjyHV"
      },
      "outputs": [],
      "source": [
        "# 🔧 Add code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXnnuRdw1AOk"
      },
      "source": [
        "🔧 Add comment here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcZsF_7whfHE"
      },
      "source": [
        "# 🔧 Part 7: Reflection (In 100 words or less per question)\n",
        "\n",
        "\n",
        "### Questions:\n",
        "1. What business-relevant topics were users discussing the most?\n",
        "2. How useful would this clustering approach be for a company’s marketing or strategy team?\n",
        "3. How would the topics shift if you pulled data from a different subreddit like `r/technology` or `r/antiwork`?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLmO9zwWftml"
      },
      "source": [
        "🔧 Add comments here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Your Notebook to Submit in Canvas\n",
        "- Use the instructions from Lab 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!jupyter nbconvert --to html \"lab_14_LastnameFirstname.ipynb\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
