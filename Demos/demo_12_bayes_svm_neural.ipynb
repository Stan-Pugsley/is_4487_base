{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS4487 Week 12 - Practice Code\n",
    "\n",
    "This notebook is designed to help you follow along with the **Week 12 Reading** on advanced data modeling techniques.\n",
    "It includes practical code examples for the three machine learning models discussed:\n",
    "\n",
    "- **Naïve Bayes Classifier** – For spam email detection  \n",
    "- **Support Vector Machine (SVM)** – For fraud detection  \n",
    "- **Neural Network** – For fraud detection using deep learning\n",
    "\n",
    "Each section contains short explanations and annotated code that reflect the steps in the reading.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Stan-Pugsley/is_4487_base/blob/main/Demos/demo_12_bayes_svm_neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes: A Probabilistic Classifier\n",
    "\n",
    "Naïve Bayes is a simple and efficient supervised machine learning algorithm based on Bayes' Theorem.\n",
    "It's especially effective in text classification problems like spam filtering, where it calculates the probability\n",
    "of a class based on feature presence.\n",
    "\n",
    "#### Context: Spam Email Detection\n",
    "In this example, we use a small set of email text samples. Each email is labeled as either:\n",
    "- `1` for **Spam** (e.g., containing promotional keywords like \"win\", \"free\", \"prize\")\n",
    "- `0` for **Not Spam** (e.g., work-related messages like \"project\", \"meeting\")\n",
    "\n",
    "The model learns the patterns of words that typically appear in spam vs. non-spam emails and makes predictions accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for text processing and Naïve Bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample email dataset and their corresponding labels (1 = Spam, 0 = Not Spam)\n",
    "emails = [\n",
    "    \"Win a free lottery ticket now\",\n",
    "    \"Limited offer! Claim your free prize today\",\n",
    "    \"Urgent! Update your bank account details\",\n",
    "    \"Meeting tomorrow at 10 AM\",\n",
    "    \"Project deadline extended to next week\",\n",
    "    \"Hey, can you send me the report?\",\n",
    "    \"Congratulations! You won a vacation trip\",\n",
    "    \"Special discount just for you, act fast!\",\n",
    "    \"Let's schedule a call for the project discussion\",\n",
    "    \"Can we reschedule our meeting to next Monday?\"\n",
    "]\n",
    "labels = [1, 1, 1, 0, 0, 0, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into numeric vectors using Bag of Words (CountVectorizer)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Split the dataset into training and testing sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a Multinomial Naïve Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and evaluate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print test results and model performance\n",
    "print(\"=== Spam Email Classifier Results ===\\n\")\n",
    "for email, pred in zip(vectorizer.inverse_transform(X_test), y_pred):\n",
    "    print(f\"Email Words: {email} --> Predicted as {'Spam' if pred == 1 else 'Not Spam'}\")\n",
    "\n",
    "print(\"\\nModel Accuracy:\", round(accuracy * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM): Finding the Optimal Hyperplane\n",
    "\n",
    "SVM is a powerful classification algorithm that works by finding a hyperplane that best separates the classes.\n",
    "It performs well on high-dimensional data and can use kernel functions to handle non-linear boundaries.\n",
    "\n",
    "#### Context: Fraud Detection (Structured Data)\n",
    "This example uses a synthetic dataset of 1,000 credit card transactions. Each transaction is labeled as:\n",
    "- `0` for **Legitimate** transactions (lower amounts, moderate frequency, older accounts)\n",
    "- `1` for **Fraudulent** transactions (high amounts, frequent activity, newly opened accounts)\n",
    "\n",
    "SVM tries to distinguish between the two classes based on three features: transaction amount, frequency, and account age.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data generation, visualization, and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Generate synthetic dataset for fraud detection\n",
    "np.random.seed(42)\n",
    "legit_transactions = np.random.normal(loc=[50, 5, 24], scale=[15, 2, 12], size=(900, 3))\n",
    "fraud_transactions = np.random.normal(loc=[500, 20, 3], scale=[100, 5, 1], size=(100, 3))\n",
    "X = np.vstack((legit_transactions, fraud_transactions))\n",
    "y = np.array([0]*900 + [1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preapare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset and normalize features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM with RBF kernel\n",
    "svm = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"Legit\", \"Fraud\"], yticklabels=[\"Legit\", \"Fraud\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - SVM Fraud Detection\")\n",
    "plt.show()\n",
    "\n",
    "# Print classification report and accuracy\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Legit\", \"Fraud\"]))\n",
    "print(f\"Model Accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks: Learning Complex Patterns\n",
    "\n",
    "Neural Networks consist of layers of interconnected nodes (neurons) that transform inputs through weighted connections.\n",
    "They are highly flexible and powerful for detecting complex relationships, especially in structured and unstructured data.\n",
    "\n",
    "#### Context: Fraud Detection Using Deep Learning\n",
    "Using the same synthetic dataset as the SVM example, this neural network attempts to learn patterns that indicate fraudulent transactions.\n",
    "Neural networks are especially useful when the relationship between input features and outcomes is non-linear or more complex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required TensorFlow modules for neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Reuse existing dataset and normalize again for neural network\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model with one hidden layer\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(3,)),  # Input layer with 3 features\n",
    "    Dense(8, activation='relu'),                     # Hidden layer\n",
    "    Dense(1, activation='sigmoid')                   # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model with optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store training history\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"Legit\", \"Fraud\"], yticklabels=[\"Legit\", \"Fraud\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Neural Network Fraud Detection\")\n",
    "plt.show()\n",
    "\n",
    "# Display evaluation metrics and learning curves\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Legit\", \"Fraud\"]))\n",
    "print(f\"Model Accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "# Plot training/validation loss and accuracy over epochs\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
